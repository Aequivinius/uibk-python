{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "freelance-positive",
   "metadata": {},
   "source": [
    "## Shakespeare's favorite words\n",
    "Let's find out what Shakespeares favorite (most frequently used) words were by looking at a corpus of his texts. At the end, a .csv-file will be created to store the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-priority",
   "metadata": {},
   "source": [
    "### Access the files of the corpus\n",
    "Create a list with the paths of all files in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "tight-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 42 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"../exercise-5/corpus\"\n",
    "path_corpus = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "print(f\"The corpus contains {len(path_corpus)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-freight",
   "metadata": {},
   "source": [
    "### Find and normalize tokens\n",
    "Create a list that contains lists of tokens, one for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "studied-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for 42 texts are normalized.\n"
     ]
    }
   ],
   "source": [
    "all_normalized_tokens = []\n",
    "\n",
    "for file_path in path_corpus:\n",
    "    f = open(file_path)\n",
    "    content = f.read()\n",
    "    tokens = content.split()\n",
    "    normalized_tokens = [token.lower().strip(\",.!?[]()=-...\") for token in tokens]\n",
    "    for token in normalized_tokens:\n",
    "        if len(token) == 0:\n",
    "            normalized_tokens.remove(token)\n",
    "    all_normalized_tokens.append(normalized_tokens)\n",
    "\n",
    "\n",
    "print(f\"Tokens for {len(all_normalized_tokens)} texts are normalized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-thread",
   "metadata": {},
   "source": [
    "### Create a dictionary with counts per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adapted-rotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted 35749 different tokens.\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for token_list in all_normalized_tokens:\n",
    "    for token in token_list:\n",
    "        counts[token] = counts.get(token, 0) + 1\n",
    "\n",
    "print(f\"Counted {len(counts)} different tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-macedonia",
   "metadata": {},
   "source": [
    "### Sort tokens by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "monthly-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 35749 tokens are now ordered by frequency, the five most frequent words are:\n",
      "\n",
      "the:\t29236\n",
      "and:\t28282\n",
      "to:\t21904\n",
      "i:\t21122\n",
      "of:\t18427\n"
     ]
    }
   ],
   "source": [
    "token_frequencies = sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(f\"The {len(token_frequencies)} tokens are now ordered by frequency, the five most frequent words are:\\n\")\n",
    "for token, count in token_frequencies[:5]:\n",
    "    print(token, count, sep=\":\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-albuquerque",
   "metadata": {},
   "source": [
    "### Store the frequencies in a csv-file\n",
    "This file is stored in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "british-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an outputfile called 'shakespeares_favorites.csv' where the frequencies are listed.\n",
      "\n",
      "Format of the output file:\n",
      "\n",
      "1,the,29236,0.03041915295415173\n",
      "\n",
      "2,and,28282,0.029426545486705407\n",
      "\n",
      "3,to,21904,0.022790433927614567\n",
      "\n",
      "4,i,21122,0.021976787135640746\n",
      "\n",
      "5,of,18427,0.01917272306355705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_tokens = 0\n",
    "rank = 0\n",
    "         \n",
    "output_file = \"shakespeares_favorites.csv\"\n",
    "\n",
    "for token, tokencount in token_frequencies:\n",
    "    sum_tokens += tokencount   # calculate total sum of tokens for frequcency below\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for token, tokencount in token_frequencies:\n",
    "        rank = rank + 1\n",
    "        f.write(f\"{rank},{token},{tokencount},{tokencount/sum_tokens}\\n\")\n",
    "\n",
    "print(f\"Created an outputfile called '{output_file}' where the frequencies are listed.\\n\\nFormat of the output file:\\n\")\n",
    "with open(output_file, \"r\") as f:\n",
    "    content = f.readlines()\n",
    "for line in content[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-doctor",
   "metadata": {},
   "source": [
    "### ... so what are Shakespeare's favorite words then?\n",
    "His favorite words were stopwords like \"the\", \"and\", \"to\", and \"I\" - which is no suprise, as these are the most frequently used words in the English language. To find his \"real\" favorite words, one would need to remove these stopwords from the analysis.\n",
    "(I was curious what the result might be, so I did a quick and easy demonstration of that. I already had the stopwords-list from another course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "virtual-anderson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, we have only 35594 instead of 35749 tokens left.\n",
      "\n",
      "These are the 'new' most frequent words:\n",
      "\n",
      "thou\t5864\n",
      "will\t5233\n",
      "thy\t4329\n",
      "shall\t3837\n",
      "thee\t3309\n",
      "lord\t3078\n",
      "now\t2970\n",
      "good\t2876\n",
      "king\t2826\n",
      "sir\t2765\n",
      "come\t2527\n",
      "enter\t2506\n",
      "o\t2249\n",
      "let\t2207\n",
      "love\t2199\n"
     ]
    }
   ],
   "source": [
    "no_stopwords = []\n",
    "with open(\"stopwords.txt\") as f:\n",
    "    stopwords = f.readlines()\n",
    "stopwords = [word.strip() for word in stopwords]\n",
    "\n",
    "\n",
    "for (word, count) in token_frequencies:\n",
    "    if word not in stopwords:\n",
    "        no_stopwords.append((word, count))\n",
    "print(f\"\"\"Now, we have only {len(no_stopwords)} instead of {len(token_frequencies)} tokens left.\\n\n",
    "These are the 'new' most frequent words:\\n\"\"\")\n",
    "for (word, count) in no_stopwords[:15]:\n",
    "    print(word, count, sep='\\t')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-snake",
   "metadata": {},
   "source": [
    "As we can see, the first few tokens are still partially stopwords as Shakespeare used (now) archaic forms of pronouns etc.\n",
    "However, if we have a look at the tokens that follow, we reach more and more 'non'-stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "three-jordan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well\t2192\n",
      "hath\t2049\n",
      "man\t1947\n",
      "one\t1917\n",
      "like\t1905\n",
      "upon\t1879\n",
      "may\t1774\n",
      "make\t1770\n",
      "know\t1754\n",
      "go\t1754\n",
      "say\t1725\n",
      "yet\t1708\n",
      "us\t1704\n",
      "must\t1624\n",
      "see\t1537\n",
      "'tis\t1489\n",
      "give\t1407\n",
      "th'\t1365\n",
      "can\t1299\n",
      "first\t1295\n"
     ]
    }
   ],
   "source": [
    "for (word, count) in no_stopwords[15:35]:\n",
    "    print(word, count, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-agreement",
   "metadata": {},
   "source": [
    "### However, ...\n",
    "This is still mostly a list of highly frequent words that might be the result of any Author's work, not just Shakespeare's. So all in all, we now know that Shakespeare used many high-frequency words, but we still don't know much about his personal favorite words. To see that, a different/more complex approach would be necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
