{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "freelance-positive",
   "metadata": {},
   "source": [
    "## Shakespeare's favorite words\n",
    "Let's find out what Shakespeares favorite (most frequently used) words were by looking at a corpus of his texts. At the end, a .csv-file will be created to store the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-priority",
   "metadata": {},
   "source": [
    "### Access the files of the corpus\n",
    "Create a list with the paths of all files in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tight-bowling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus contains 42 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"../exercise-5/corpus\"\n",
    "path_corpus = [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "print(f\"The corpus contains {len(path_corpus)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-freight",
   "metadata": {},
   "source": [
    "### Find and normalize tokens\n",
    "Create a list that contains lists of tokens, one for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "studied-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for 42 texts are normalized.\n"
     ]
    }
   ],
   "source": [
    "all_normalized_tokens = []\n",
    "\n",
    "for file_path in path_corpus:\n",
    "    f = open(file_path)\n",
    "    content = f.read()\n",
    "    tokens = content.split()\n",
    "    normalized_tokens = [token.lower().strip(\",.!?[]()=-...\") for token in tokens]\n",
    "    for token in normalized_tokens:\n",
    "        if len(token) == 0:\n",
    "            normalized_tokens.remove(token)\n",
    "    all_normalized_tokens.append(normalized_tokens)\n",
    "\n",
    "\n",
    "print(f\"Tokens for {len(all_normalized_tokens)} texts are normalized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-thread",
   "metadata": {},
   "source": [
    "### Create a dictionary with counts per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adapted-rotation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted 35749 different tokens.\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for token_list in all_normalized_tokens:\n",
    "    for token in token_list:\n",
    "        counts[token] = counts.get(token, 0) + 1\n",
    "\n",
    "print(f\"Counted {len(counts)} different tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-macedonia",
   "metadata": {},
   "source": [
    "### Sort tokens by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "monthly-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 35749 tokens are now ordered by frequency, the five most frequent words are:\n",
      "\n",
      "the:\t29236\n",
      "and:\t28282\n",
      "to:\t21904\n",
      "i:\t21122\n",
      "of:\t18427\n"
     ]
    }
   ],
   "source": [
    "token_frequencies = sorted(counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(f\"The {len(token_frequencies)} tokens are now ordered by frequency, the five most frequent words are:\\n\")\n",
    "for token, count in token_frequencies[:5]:\n",
    "    print(token, count, sep=\":\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-albuquerque",
   "metadata": {},
   "source": [
    "### Store the frequencies in a csv-file\n",
    "This file is stored in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "british-monroe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an outputfile called shakespeares_favorites.csv where the frequencies are listed.\n",
      "\n",
      "Format of the output file:\n",
      "1,the,29236,0.03041915295415173\n",
      "\n",
      "2,and,28282,0.029426545486705407\n",
      "\n",
      "3,to,21904,0.022790433927614567\n",
      "\n",
      "4,i,21122,0.021976787135640746\n",
      "\n",
      "5,of,18427,0.01917272306355705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_tokens = 0\n",
    "rank = 0\n",
    "         \n",
    "output_file = \"shakespeares_favorites.csv\"\n",
    "\n",
    "for token, tokencount in token_frequencies:\n",
    "    sum_tokens += tokencount   # calculate total sum of tokens for frequcency below\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    for token, tokencount in token_frequencies:\n",
    "        rank = rank + 1\n",
    "        f.write(f\"{rank},{token},{tokencount},{tokencount/sum_tokens}\\n\")\n",
    "\n",
    "print(f\"Created an outputfile called {output_file} where the frequencies are listed.\\n\\nFormat of the output file:\")\n",
    "with open(output_file, \"r\") as f:\n",
    "    content = f.readlines()\n",
    "for line in content[:5]:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
