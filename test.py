# def tokenize():
#     complete_string = "hallo , , ,,,,,, , -du, drt --  dEr?"
#     tokens = []
#     normalized_tokens = []
    
#     tokens = complete_string.split()
#     print(normalized_tokens)

#     for token in tokens:
#         normalized_tokens.append(token.lower().strip(",;.!?[]()=--"))

#     while ("" in normalized_tokens):
#         normalized_tokens.remove("")
#     return normalized_tokens

# print(tokenize())
# counts = {"venator" : 2}
# counts["venator"] = counts["venator"] + 1
# print(counts)
# lel = ["ha", "ha", "ha", 4]

# print(len(lel))

# lists = [[1,7], [2,4]]
# print(lists[1])
# for list in lists[1]:
#     print(list)

# print("Hallo, " + str(2021))

# uno = [4,4]
# duo = [4,4]
# if uno == duo:
# print("equal")

# import sys
# import .exercise-6.helper
# import csv